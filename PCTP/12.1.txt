1
00:00:00,780 --> 00:00:06,090
呃

2
00:00:11,580 --> 00:00:16,890
好，今天由我给大家介绍一下tidb server的性能调优，及它的一些相关

3
00:00:16,895 --> 00:00:22,080
原理，我叫于帅鹏，我是PingCAP tidb架构组的研发组长


4
00:00:22,085 --> 00:00:25,740
本课的概要主要是第一是课程背景


5
00:00:25,860 --> 00:00:31,140
帮助大家分析一下，tidb server的本身的一些性能问题，以及使用者需要具备一些常规的技能

6
00:00:31,650 --> 00:00:36,960
学习目标是了解tidb server性能相关的原理，并且学会分析性能


7
00:00:36,965 --> 00:00:37,560
瓶颈

8
00:00:38,580 --> 00:00:42,600
主要是面向我们零基础的这些学员

9
00:00:43,020 --> 00:00:46,470
然后知识点大纲，我画了四个

10
00:00:46,680 --> 00:00:51,990
部分第一是性能相关的基本原理，第二是监控项介绍，第三是慢日志

11
00:00:51,995 --> 00:00:54,840
介绍，第四是性能参数调优

12
00:00:55,440 --> 00:01:00,450
基本逻辑就是第一部分先介绍我们这个tidb server一些性能的基本原理

13
00:01:00,660 --> 00:01:01,590
然后呢？

14
00:01:01,740 --> 00:01:07,050
再就是介绍这个可能是我们从监控项，或者是从慢日志这

15
00:01:07,055 --> 00:01:10,110
两个角度看怎么发现这个性能问题

16
00:01:10,410 --> 00:01:15,720
发现性能问题之后，最后一部分就是介绍这个怎么进行性能调优？然后把

17
00:01:15,725 --> 00:01:17,040
这个性能调上去

18
00:01:19,350 --> 00:01:23,820
下面看一下第一部分，这个性能相关的原理基本介绍，然后

19
00:01:24,030 --> 00:01:29,340
首先看一下这个tidb的整体架构，就是说，本节课讲

20
00:01:29,345 --> 00:01:32,130
tidb，或者是tidb这部分指的是这个

21
00:01:32,220 --> 00:01:33,240
呃

22
00:01:33,300 --> 00:01:38,610
左下这边儿这个绿色部分，就是tidb server这部分，是整个tidb整体架构的

23
00:01:38,615 --> 00:01:39,480
一部分

24
00:01:39,990 --> 00:01:44,130
就是相对于其他的这个TiKV，还有右边这些TiSpark都不是本

25
00:01:44,135 --> 00:01:45,540
节课重要介绍内容

26
00:01:47,550 --> 00:01:52,860
tidb本身自己是这个性能相关使用的资源主要是有这个CPU、内

27
00:01:52,865 --> 00:01:55,170
内存，还有网络，还有锁这四个点

28
00:01:55,440 --> 00:02:00,750
因为本身tidb是一个无状态的架构，而且他本身并不需要写这个磁盘

29
00:02:00,930 --> 00:02:05,940
只有这个系统日志会写磁盘，所以说这个跟IO这个没有什么关系

30
00:02:07,650 --> 00:02:11,220
先介绍一下这个资源部分，先看一下CPU

31
00:02:11,370 --> 00:02:12,150


32
00:02:12,270 --> 00:02:13,110
CPU

33
00:02:13,380 --> 00:02:18,690
是tidb server主要消耗的资源，就是说从这个右边这样的图可以看到

34
00:02:18,780 --> 00:02:20,400
这个SQL

35
00:02:20,460 --> 00:02:22,050
走过tidb的一个全过程

36
00:02:22,140 --> 00:02:24,690
从SQL进来之后，先被解析成AST

37
00:02:24,840 --> 00:02:29,160
就是抽象语法树，然后呢，我们再进行逻辑优化，然后物理优化

38
00:02:29,430 --> 00:02:34,740
物理优化这时候呢，可能会使用这个统计信息进行一些更优的一些物理优化

39
00:02:34,745 --> 00:02:36,330
这里边我们有一个自己的代价模型

40
00:02:36,630 --> 00:02:38,190
最终会生成

41
00:02:38,430 --> 00:02:43,740
向下发的这个执行计划，执行计划发下去之后其实就跟tidb本身这个

42
00:02:43,890 --> 00:02:47,550
自己的资源使用关系不大了，主要是向TiKV发送请求

43
00:02:48,570 --> 00:02:53,880
下面这个图其实就是我用go这个自带的这个pprof

44
00:02:54,090 --> 00:02:57,150
工具对tidb server进行的一个profile

45
00:02:57,510 --> 00:03:02,820
然后它可以看到整个上面这个每个过程当中到底使用了多少CPU，哪部分占比最大

46
00:03:02,825 --> 00:03:07,980
哪部分占比最小，所以说下图这是一个用 go prof

47
00:03:08,040 --> 00:03:13,230
自己抓出来的火焰图，大家自己在进行性能调优的时候，也可以进行这样的抓取

48
00:03:13,590 --> 00:03:18,900
然后抓取出来文件，大家如果自己看也可以，或者是拿给这个

49
00:03:18,905 --> 00:03:21,960
tidb相关同事进行技术支持也可以

50
00:03:23,640 --> 00:03:25,320
下面是这个内存部分

51
00:03:25,350 --> 00:03:27,870
呃，内存主要是这个

52
00:03:28,230 --> 00:03:29,490
对于

53
00:03:29,580 --> 00:03:34,890
golang来讲是一个自带gc的一个语言，我们用golang写的tidb嘛，所以说本身内

54
00:03:34,895 --> 00:03:40,110
存分配次数如果过多的话，最终其实还是会反映到CPU上，所以说内存的这个

55
00:03:40,115 --> 00:03:43,200
资源使用，最终其实也会反映在CPU的使用

56
00:03:44,670 --> 00:03:46,050
然后是网络

57
00:03:46,320 --> 00:03:51,630
这里我列了一下，就是大家常见的一些，就是尝试性的一些经验值吧，就比如说

58
00:03:51,635 --> 00:03:56,940
访问 L1 Cache 需要时间多少，然后访问这个内存需要的时间多少，这里

59
00:03:56,945 --> 00:04:02,250
我重点标注了两行，一个是顺序读1M内存所需要时间

60
00:04:03,060 --> 00:04:05,070
然后另一个是顺序

61
00:04:05,100 --> 00:04:10,410
从网络读1M所需要时间，大家可以看到这两个值差别还是非常大的

62
00:04:11,130 --> 00:04:16,440
也就是说，如果是进行一次简单内存访问和进行一次简单的网络访问的话，其实它们

63
00:04:16,445 --> 00:04:19,590
所花费的时间差距是非常非常大的

64
00:04:19,800 --> 00:04:21,840
这里可以看到大概有四倍的差距

65
00:04:22,080 --> 00:04:25,500
现在我们介绍一下网络资源这部分

66
00:04:25,740 --> 00:04:31,050
因为上面我们介绍就是说，通过网络进行顺序访问的话，实际上是比通过内存

67
00:04:31,055 --> 00:04:36,360
访问要慢非常多的，但是tidb本身是一个对网络依赖非常重的一个组件

68
00:04:36,420 --> 00:04:38,820
首先我们看一下就是说

69
00:04:38,970 --> 00:04:44,280
在启动和提交事务的时候，我们分别会向PD

70
00:04:44,285 --> 00:04:45,960
获取两次tso

71
00:04:46,200 --> 00:04:49,710
我们称这两次，一次叫 start ts，一次叫 commit ts

72
00:04:49,800 --> 00:04:55,110
然后每个事务在启动时都会通过网络获取 start ts，然后呢？

73
00:04:55,380 --> 00:04:59,100
如果是自动提交的点查，比如说像这个

74
00:04:59,105 --> 00:05:04,410
例子它是一个例外，它是不需要获取这个start ts的，就是说这个 select * from t

75
00:05:04,415 --> 00:05:08,310
where i=1，这个条件i呢是一个 int 的主键

76
00:05:08,640 --> 00:05:13,860
然后对于只读的事务，它是不需要获取这个commit ts，它没有任何东西可以提交

77
00:05:14,340 --> 00:05:19,650
对于非只读的事务呢？其实他最终提交的时候还会获取ts，这次叫 commit ts 的

78
00:05:19,655 --> 00:05:20,670


79
00:05:21,690 --> 00:05:27,000
然后kv提交呢，其实是一个分布式并发操作，但是呢，它也是

80
00:05:27,005 --> 00:05:32,310
需要走很多次网络，就是因为我们的数据和索引是分开存的，所以kv的commit需要走数据

81
00:05:32,315 --> 00:05:33,150
和索引两部分

82
00:05:33,450 --> 00:05:35,850
然后第三部分就是raft

83
00:05:36,090 --> 00:05:41,400
因为我们本身TiKV是用raft算法实现一个高可用，所以说这里有一个

84
00:05:41,405 --> 00:05:46,710
raft协议，也就是说他要达到大多数一致，这个是在tikv里做的，但是也是

85
00:05:46,715 --> 00:05:49,410
记在整个提交时间里面的

86
00:05:50,940 --> 00:05:54,870
我们可以看一下下面这个例子，就是我如果 create table t

87
00:05:54,900 --> 00:05:57,210
然后这个是 i int unique key

88
00:05:57,570 --> 00:06:02,880
然后，这种情况下呢，就是说我会创建一张表，这张表里

89
00:06:02,885 --> 00:06:08,190
有一个int的主键，int的唯一索引，然后呢，这时候我 insert into 

90
00:06:08,610 --> 00:06:10,680
values 1，这时候插入一行数据

91
00:06:10,860 --> 00:06:14,040
然后我简单做下面这个 update t set i=2

92
00:06:14,520 --> 00:06:19,830
然后简单这么一条语句，其实可能在mysql里做的事情还是比较简单的，就是整个过程

93
00:06:19,835 --> 00:06:25,140
相对于链路来说，比较短一些，但是在tidb里呢，他分了下面几个

94
00:06:25,145 --> 00:06:26,010
阶段

95
00:06:26,250 --> 00:06:31,560
下面我们主要看一下，就第一个阶段，这里有一个启动阶段，这里获取了一个start ts

96
00:06:31,565 --> 00:06:33,150
我们上面讲的那个地方

97
00:06:33,360 --> 00:06:38,670
然后第二阶段就是读阶段，读阶段实际它要读一次索引，因为这个表示

98
00:06:38,675 --> 00:06:43,980
有索引，也是有这个表的数据的，所以说它要先去把索引的数据读出来，然后

99
00:06:43,985 --> 00:06:45,330
再读表里的数据

100
00:06:45,930 --> 00:06:46,830
相当于

101
00:06:46,980 --> 00:06:49,050
大家常讲的这种回表操作

102
00:06:49,170 --> 00:06:50,790
第三个阶段是提交阶段

103
00:06:50,970 --> 00:06:56,280
其实我读了这一行之后我要进行提交，提交的时候，当然提交之前，我要先把这

104
00:06:56,520 --> 00:06:59,610
值先计算好嘛，就是别我把这个1要

105
00:06:59,640 --> 00:07:00,660
update成2

106
00:07:00,665 --> 00:07:05,790
这时候呢，首先还要获取一次这个commit ts，然后呢，还要进行这个

107
00:07:05,820 --> 00:07:11,130
tikv的两次写入，两次写入分别是因为我们对应两个region，有这个数据段和这个索引段

108
00:07:11,490 --> 00:07:13,440
然后还有下面这个

109
00:07:13,590 --> 00:07:18,900
因为我们要进行一次两阶段提交，因为tidb是一个分布式的一个数据库，所以说

110
00:07:19,380 --> 00:07:24,690
只是进行简单的写入是不行的，不是我写了之后立刻进行提交，而是我要进行一次预提交

111
00:07:24,695 --> 00:07:27,780
然后还要进行最后的确认的提交，这两个部分

112
00:07:28,020 --> 00:07:30,390
这个整个两次，还要再乘以2

113
00:07:30,690 --> 00:07:34,440
然后刚才讲的raft部分，tikv之间还有一个raft

114
00:07:34,530 --> 00:07:38,220
这里面是包含了写入key的个数，在这里呢，是两个key

115
00:07:38,400 --> 00:07:42,030
然后呢乘以它的副本数除以2加1，就是大多数的一致

116
00:07:42,540 --> 00:07:45,300
然后这个都是必要，必须要等待的时间

117
00:07:45,330 --> 00:07:50,640
当时其实可能最终总的访问时间比较多，因每一个副本都需要这样一次访问请求

118
00:07:50,645 --> 00:07:53,670
等待至少一个是一个这个大多数的这个请求

119
00:07:54,420 --> 00:07:57,900
右图其实就介绍了这个两阶段提交到底是怎么回事

120
00:07:58,140 --> 00:08:03,450
两阶段提交这个协议呢？其实是一个分布式数据里主要的一个，大家用来

121
00:08:03,455 --> 00:08:08,760
进行这种分布式事务的协议，当然其实理论上还有一些，比如说三阶段提交这样

122
00:08:08,765 --> 00:08:10,590
算法，在实际上

123
00:08:10,860 --> 00:08:16,170
应用中的话，还是两阶段提交是一个最主要的一个协议，两阶段提交主要

124
00:08:16,175 --> 00:08:21,480
过程就是从一个协调者，这个协调者，对于tidb架构来讲

125
00:08:21,485 --> 00:08:26,790
这就是说，上层的这个tidb节点相当于这个协调者，然后下层的这个每个tikv节点

126
00:08:26,795 --> 00:08:27,840
相当于

127
00:08:27,845 --> 00:08:28,950
这个参与者

128
00:08:29,550 --> 00:08:30,390
然后

129
00:08:30,480 --> 00:08:35,790
整个事物提交的时候需要经历两个阶段，第一阶段先发出两个prepare，先让这两个参与者

130
00:08:35,795 --> 00:08:37,650
都这个数据prepare好

131
00:08:37,770 --> 00:08:40,200
大家都同意的事情况下，就是每个

132
00:08:40,380 --> 00:08:44,550
几个参与者都返回prepare是成功的情况下，我整个事务才能提交

133
00:08:44,880 --> 00:08:48,480
然后呢，这时候都返回到上层提交者节点之后呢

134
00:08:48,540 --> 00:08:49,650
会把这个

135
00:08:49,740 --> 00:08:50,550
呃

136
00:08:50,670 --> 00:08:55,980
commit请求发下去，然后让两个这个参与者的把事务提交上去，这样才能保证这个事务

137
00:08:55,985 --> 00:08:56,610
原子性

138
00:09:00,570 --> 00:09:05,880
下面讲一下这个最后一个资源，最后一个资源就是锁这部分，在乐观事务下，其实只有

139
00:09:06,030 --> 00:09:11,340
提交时会需要加锁，所以大部分时间整个事务间是不需要加锁的，但是提交

140
00:09:11,345 --> 00:09:16,650
的时候加锁主要是在这个prewrite之后，prewrite以后我要把这个，我需要

141
00:09:16,655 --> 00:09:19,650
从预提交的事务加上锁，然后当commit时候，我去放锁

142
00:09:20,130 --> 00:09:25,440
然后prewrite完了之后，这个事务实际上可能会等一段时间才会遇到这个commit请求

143
00:09:25,445 --> 00:09:28,530
所以说，在这个期间内，所有的请求都是

144
00:09:28,800 --> 00:09:34,110
有可能会被阻塞住的，有可能阻塞住有两种情况，一种是读写冲突，就是说在这个prewrite期间

145
00:09:34,115 --> 00:09:36,900
有任何的读请求，都会需要等锁

146
00:09:37,230 --> 00:09:41,940
然后第二种就是有写请求，就是有另外的prewrite请求过来，它也需要

147
00:09:41,970 --> 00:09:45,420
进行写的话，它也需要有这种写的等待，就是写冲突

148
00:09:45,480 --> 00:09:46,170
写写冲突

149
00:09:46,560 --> 00:09:49,800
这样一个冲突，指的是主要是

150
00:09:50,820 --> 00:09:53,610
就是说正好读写请求遇到了正在提交的事务

151
00:09:54,240 --> 00:09:57,090
然后在tidb里跟那个就是

152
00:09:57,095 --> 00:10:02,400
传统的单机数据库里比较不一样的一个地方就是我们有这种backoff的机制，单机数据库里

153
00:10:02,405 --> 00:10:07,710
等锁的时候，他可能会用一种这种，系统自带这种，就是锁的实现进行

154
00:10:07,715 --> 00:10:13,020
等锁，而tidb实现的是这种backoff机制，其实也就是说，它是不断地进行请求，请求之后，然后

155
00:10:13,560 --> 00:10:14,910
tikv告诉我这个

156
00:10:15,930 --> 00:10:21,240
此处有这可以有锁，所以你不能这个进行读或者写，然后呢，就会把这个

157
00:10:21,330 --> 00:10:23,160
信息返回上来，然后tidb

158
00:10:23,250 --> 00:10:28,560
就得到这个返回请求，然后呢？我知道这个时候是有锁的，我需要等一段时间

159
00:10:29,160 --> 00:10:34,470
等一段时间之后呢，我再去请求，然后对方这时候，如果这时候还是有锁的话，我还要

160
00:10:34,740 --> 00:10:40,050
在返回给我还需要再等一段时间，直到这个超时为止，所以它是一个不断增加等待时间

161
00:10:40,055 --> 00:10:41,790
直到超时的这么一种机制

162
00:10:41,820 --> 00:10:47,130
当然，如果你发现后续请求发现这个锁没了，然后就可以正常进行读写了，这个

163
00:10:47,135 --> 00:10:50,580
就是相当于锁已经消失了，就可以正常的进行操作了，这个

164
00:10:50,850 --> 00:10:52,140
我们就不再继续介绍

165
00:10:53,010 --> 00:10:55,740
然后下面是不同负载对于性能影响

166
00:10:56,280 --> 00:10:57,540
一个是这个

167
00:10:58,020 --> 00:11:03,330
读请求，读请求分为这么几部分，一部分是这个tidb内部的这个计算

168
00:11:09,150 --> 00:11:10,740
内部计算就是sql自己优化，优化完之后，我们自己执行的部分，就是怎么样把数据拼装出来，然后返回给客户端

169
00:11:10,800 --> 00:11:12,960
然后第二部分是kv读

170
00:11:13,410 --> 00:11:17,460
kv读就是我直接向tikv进行这种 kv get 这种操作

171
00:11:18,240 --> 00:11:23,550
kv读什么时候会发生呢？就是在这种通过tikv这种 storage readpool 进行点查

172
00:11:23,640 --> 00:11:25,320
或者点更新或者删除的时候会

173
00:11:25,325 --> 00:11:29,910
出现这种kv读，这是我们2.1做的一个优化，就是当我们

174
00:11:29,940 --> 00:11:35,250
这个可以确定这行只涉及到一个kv对的时候，我们可以进行这种操作，然后这种

175
00:11:35,255 --> 00:11:40,560
操作的实际上，略过大部分的这个执行逻辑，实际上走一个最短路径

176
00:11:40,860 --> 00:11:43,710
内部我们称它为叫 fast pass

177
00:11:44,520 --> 00:11:46,470
这种读是效率最高的一种读

178
00:11:47,070 --> 00:11:52,380
第三个读是coprocessor读，coprocessor读就是绝大部分，如果是不能满足上面这个

179
00:11:52,385 --> 00:11:54,900
kv读条件的时候，我们都会走coprocessor读

180
00:11:55,260 --> 00:12:00,570
都会通过tikv coprocessor读把数据读上来，这种读就是能适用于最多的情况，但是

181
00:12:00,575 --> 00:12:05,190
他也是，相对于讲，处理的流程逻辑是最复杂的

182
00:12:06,480 --> 00:12:09,540
下面是写请求，写请求就是这个

183
00:12:09,660 --> 00:12:12,900
包含了这个tidb内部计算，还有事务提交两部分

184
00:12:13,410 --> 00:12:18,720
tidb内部计算的相对比较简单，就是当我这个，比如进行了一个update的操作，我可能要

185
00:12:18,725 --> 00:12:24,030
这个读上来的数据进行一个变换，把它变换成我需要写

186
00:12:24,035 --> 00:12:24,720
入的值

187
00:12:24,870 --> 00:12:30,180
然后第二部分就是事务提交了，事务提交就是刚刚我讲的，就是他也需要有做了很多的事

188
00:12:30,185 --> 00:12:32,280
完成两阶段提交的过程

189
00:12:35,250 --> 00:12:40,230
然后读请求这个，我们讲一下，跟mysql差异吧，然后就是第一部分就是说

190
00:12:40,260 --> 00:12:43,740
tidb中其实是没有任何的读缓存的

191
00:12:44,310 --> 00:12:49,620
他不像mysql，mysql其实是有很多 buffer pool的，像这个oracle等其他数据库其实也是有buffer pool的

192
00:12:49,890 --> 00:12:52,680
tidb自身上的这个组件是没有这个缓存这部分

193
00:12:52,685 --> 00:12:57,120
所以说我如果要读任何数据我都需要从tikv去把数据拉上来

194
00:12:58,140 --> 00:13:02,670
然后但是tikv中，tidb中是有这个事务的私有写缓存

195
00:13:02,970 --> 00:13:06,360
事务写缓存数据会暂时缓存在这个事务内部

196
00:13:06,750 --> 00:13:12,060
然后缓存在事务内这部分数据呢，实际上，这个是事务内部的自己一个私有buffer，所以

197
00:13:12,065 --> 00:13:14,880
这也是一定程度上保证了一个事务隔离性

198
00:13:15,030 --> 00:13:17,790
然后当这个读请求也

199
00:13:17,940 --> 00:13:21,390
读到缓存的数据的时候，会与tikv上的数据进行合并

200
00:13:21,750 --> 00:13:23,490
可以看下下面这个简单的例子

201
00:13:24,660 --> 00:13:29,970
就比如说我这个是一个空表，我只是写一行数据

202
00:13:30,000 --> 00:13:35,310
insert into test表，然后values 1，这时候我begin，如果insert into values 

203
00:13:35,315 --> 00:13:37,650
2的时候，这时候实际上

204
00:13:37,800 --> 00:13:43,110
虽然我现在是在一个session里做的，但是我前面这个insert 1已经写入tikv了，然后

205
00:13:43,115 --> 00:13:44,820
tidb也不会缓存这个1

206
00:13:44,825 --> 00:13:50,130
如果说到下面我再去进行select的时候，实际上我还是会去tikv去

207
00:13:50,135 --> 00:13:52,050
把这个数据捞上来

208
00:13:52,440 --> 00:13:57,750
然后呢，同时呢，我还会把事务自己的已经写入这个2，也读上来，这时候

209
00:13:57,755 --> 00:13:58,560
会把

210
00:13:58,565 --> 00:14:02,070
事务里的1和这个tikv的1进行合并

211
00:14:03,210 --> 00:14:08,520
这就是造成event，就是说像大家都知道tidb实际上在这种情况下就变成一个单点

212
00:14:08,760 --> 00:14:14,070
就是说的我们可能下面有很多的节点，一个tikv集群，所以

213
00:14:14,075 --> 00:14:19,380
我这个读请求，其实我是可以让多个tikv并发执行的，但是事务内部自身的

214
00:14:19,385 --> 00:14:21,360
数据其实只有tidb自己

215
00:14:21,930 --> 00:14:23,400
能够处理的了

216
00:14:23,490 --> 00:14:28,800
因为它只在tidb内部自己存在，只在事务自己存在，所以这个合并过程的是

217
00:14:28,805 --> 00:14:31,920
实际上是仅在tidb单点内部处理

218
00:14:32,220 --> 00:14:37,230
事务内的数据如果是过大的话，其实会大大降低这个，最终这个读性能

219
00:14:41,190 --> 00:14:44,520
下面我们看一下这个写请求跟这个mysql差异

220
00:14:45,000 --> 00:14:50,220
其实事务在提交前呢，tidb缓存所有这个即将提交修改，然后

221
00:14:50,310 --> 00:14:55,620
其实写请求对于这个一致性检查是有这个要求的，也是会进行大量的读，就比如

222
00:14:55,625 --> 00:14:56,310
说

223
00:14:56,370 --> 00:15:01,680
我在这个事务当中，我需要update某一行，然而update这一行之后

224
00:15:01,685 --> 00:15:03,000
update

225
00:15:03,060 --> 00:15:04,290
出来的值

226
00:15:04,440 --> 00:15:09,750
我并不知道这个值是不是合法，是不是满足这个一致性约束的，就是说我是不是满足

227
00:15:09,755 --> 00:15:12,480
他的索引是唯一的等等这些条件

228
00:15:12,930 --> 00:15:16,020
所以我要去tikv去check，去检查这个

229
00:15:16,140 --> 00:15:18,090
是不是在tikv有一样的数据

230
00:15:18,570 --> 00:15:23,760
但是tidb也做了一个额外优化，就所谓的我们的这个insert lasy check

231
00:15:24,000 --> 00:15:29,310
insert lazy check是什么意思？就是说对于insert来讲，但是这里不包括这个insert 

232
00:15:29,315 --> 00:15:31,470
ignore 和 insert on duplicate key update 这样的语句

233
00:15:31,770 --> 00:15:34,950
对于普通insert的来讲，其实我是一个延迟检查的

234
00:15:35,760 --> 00:15:37,650
什么意思呢？我们看一下右边这个例子

235
00:15:40,080 --> 00:15:43,680
这有一张表也是写入一行数据，这时候

236
00:15:43,685 --> 00:15:44,940
呃

237
00:15:45,090 --> 00:15:48,720
这个数据是1，比如说，它还是存在一个唯一索引的

238
00:15:49,020 --> 00:15:49,770
然后

239
00:15:49,890 --> 00:15:55,200
我这时候begin，再insert 1的时候，这时候mysql其实很快，他会立刻检测到

240
00:15:56,100 --> 00:16:00,360
因为他知道这个表已经存在1的数据，而且这个1已经提交过了

241
00:16:00,480 --> 00:16:05,490
然后后面在这个事务再写一个冲突k的时候，它就会进行，它会报错

242
00:16:05,820 --> 00:16:09,840
然后呢，mysql是有语句回滚的这种特性的，所以说

243
00:16:09,845 --> 00:16:15,150
它可以继续执行，比如说他在insert into values 2，这个第二条语句就是成功的，然后它提交

244
00:16:15,155 --> 00:16:17,100
的时候呢，mysql这里会提交成功

245
00:16:17,790 --> 00:16:22,740
所以整个事务来讲，从begin到commit这四条语句，实际上就只有第二条失败了

246
00:16:23,040 --> 00:16:26,130
然后第三条成功的，最终呢，其实它

247
00:16:26,160 --> 00:16:28,230
再读一下这个表的话，你会发现

248
00:16:28,440 --> 00:16:30,420
mysql可以读到两条，就是说

249
00:16:30,425 --> 00:16:33,090
insert第一个values 1，还有这个

250
00:16:33,300 --> 00:16:35,220
第二个事务写的values 2

251
00:16:36,750 --> 00:16:42,060
但是呢，tidb是不一样的，tidb先假设这个整个事务里没有能冲突的

252
00:16:42,065 --> 00:16:43,140
这个

253
00:16:43,470 --> 00:16:44,220
这个

254
00:16:44,340 --> 00:16:49,650
数据，也就是说我这里begin之后，我insert into values 1，tidb在这里不会报错

255
00:16:51,240 --> 00:16:56,550
然后呢？我再insert into values 2，然后这时候其实也不会报错，因为在事务内部并没有

256
00:16:56,555 --> 00:16:57,570
形成冲突嘛

257
00:16:57,780 --> 00:17:01,650
然后我最后commit，commit的时候呢，这时候tidb才会进

258
00:17:01,740 --> 00:17:03,240
一致性的约束检查

259
00:17:03,480 --> 00:17:05,850
然后这时候这个这时候就会报错了

260
00:17:06,570 --> 00:17:07,590
然后

261
00:17:07,595 --> 00:17:12,900
最终可能读出来的时候，在整个事务这个，因为这时候事务提交的报错嘛

262
00:17:12,905 --> 00:17:15,060
是提交报错，整个事务是需要回滚的

263
00:17:16,110 --> 00:17:19,560
检查的时候可能读数据的时候，tidb只返回1

264
00:17:20,280 --> 00:17:21,360
这是跟mysql

265
00:17:21,390 --> 00:17:22,800
差异比较大的一个地方

266
00:17:28,170 --> 00:17:32,880
第二点就是说跟mysql差异比较大的地方，就是说我们还是基于我们这个乐观事务模型吧

267
00:17:33,240 --> 00:17:35,310
就对于tikv来讲其实

268
00:17:35,850 --> 00:17:39,810
我们仅仅做一次update来讲，只是做一次读

269
00:17:40,080 --> 00:17:42,570
然后呢，提交的时候如果冲突了，怎么办呢？

270
00:17:43,200 --> 00:17:46,440
其实，乐观事务的做法就是我

271
00:17:46,950 --> 00:17:50,700
所有的检查都是在提交时候再做，提交的时候如果冲突我只能回滚

272
00:17:51,180 --> 00:17:52,110
但是呢？

273
00:17:52,140 --> 00:17:57,450
对于很多用户，其实包括很多习惯mysql，习惯使用oracle用户，他都

274
00:17:57,660 --> 00:18:01,980
会觉得这样的事务失败率太高了，所以我们做了一个事务重试的特性

275
00:18:03,240 --> 00:18:05,790
事务重试是什么意思？也就是说

276
00:18:05,880 --> 00:18:08,760
我重新获取了一次事务的start ts

277
00:18:09,030 --> 00:18:12,750
然后重新执行了事务内部所有的写操作的sql

278
00:18:13,350 --> 00:18:17,130
然后相当于把整个事务的时间向后挪了

279
00:18:18,120 --> 00:18:23,430
所有操作都算在了这个commit语句自身的执行当中，也就是说，其实对用户是无感知的

280
00:18:24,720 --> 00:18:25,800
哼

281
00:18:26,370 --> 00:18:31,680
大家看一下下面这个例子，就是说比如说我在这里begin，begin了一个事务

282
00:18:32,670 --> 00:18:34,140
然后我进行了一次update

283
00:18:34,530 --> 00:18:36,690
我把这个i=1这行，update

284
00:18:36,750 --> 00:18:39,360
然后我在另外一个session的时候呢？

285
00:18:39,960 --> 00:18:45,270
同时也进行一个update，我把那个也把这个i=1的这行也进行了

286
00:18:45,330 --> 00:18:46,080
更新

287
00:18:46,920 --> 00:18:48,060
也就是说我在

288
00:18:48,150 --> 00:18:50,970
左边这个事务的启动到提交之间

289
00:18:50,975 --> 00:18:53,280
有人把我想改的这行数据改掉了

290
00:18:54,120 --> 00:18:56,400
这就会出现一个写入冲突

291
00:18:56,580 --> 00:18:58,680
所以在这里commit的时候

292
00:19:00,120 --> 00:19:04,740
他会在tidb内部发生一个事情，就是说他初次提交，他是会

293
00:19:04,920 --> 00:19:10,020
报出来一个失败的这个log，但是这个只会存在于日志中

294
00:19:10,200 --> 00:19:11,820
并不会实际返回客户端

295
00:19:12,390 --> 00:19:17,700
然后呢，它发现这个失败之后，tidb自己把这个整个事务重试，也就是把

296
00:19:17,705 --> 00:19:19,740
begin到commit再执行一遍

297
00:19:20,850 --> 00:19:24,510
达到效果就是相当于把整个事务挪到了这个update

298
00:19:24,750 --> 00:19:25,380
i

299
00:19:25,470 --> 00:19:27,030
等于3的这个语句后面

300
00:19:27,960 --> 00:19:32,010
然后这样的话就相当于把这个写冲突这个问题给解决了

301
00:19:32,280 --> 00:19:33,420
当然这个

302
00:19:33,570 --> 00:19:38,880
对于一些对如果说对隔离性要求比较严格的一些操作来讲，这个这个

303
00:19:40,230 --> 00:19:43,800
move这个操作，实际上是违反SI这个隔离级别的

304
00:19:47,430 --> 00:19:52,740
第二部分就是我介绍一下tidb的一些监控项，我把我认为的一些跟

305
00:19:52,745 --> 00:19:58,050
性能比较相关的一些，因为前面我们讲原理嘛，性能相关的一些监控项，给大家列到这里

306
00:19:58,260 --> 00:20:03,570
一般嘛，就是说如果说大家发现有性能问题的时候，要么第一个就是

307
00:20:04,170 --> 00:20:06,900
看监控，要么一个就是看慢日志

308
00:20:07,320 --> 00:20:12,180
才会发现，这些可能说有什么性能问题，所以说先我们先介绍这个监控项

309
00:20:12,690 --> 00:20:14,160
监控项里面有这些

310
00:20:14,760 --> 00:20:16,140
后面我们一个一个看

311
00:20:16,170 --> 00:20:18,900
第一个是duration

312
00:20:19,410 --> 00:20:23,280
duration可能是大家最常会看的一个监控

313
00:20:25,140 --> 00:20:30,450
所以说，其实本身也这里也不是特别**，因为我估计每一个同学都会看

314
00:20:30,455 --> 00:20:31,950
这个对duration这个面板

315
00:20:32,250 --> 00:20:35,520
但是单独把它写出来，实际上是想告诉大家

316
00:20:35,670 --> 00:20:38,550
这个grafana的公式其实是可以任意编辑的

317
00:20:38,970 --> 00:20:39,630


318
00:20:39,720 --> 00:20:41,940
任意编辑之后，其实我可以达到一个

319
00:20:42,090 --> 00:20:44,730
更好效果，或者达到我希望达到效果

320
00:20:45,450 --> 00:20:47,220
比如这个duration的公式

321
00:20:47,640 --> 00:20:49,080
我可以在这个

322
00:20:49,320 --> 00:20:51,270
grafana面板上进行edit

323
00:20:52,140 --> 00:20:53,790
比如我加入下面这个公式

324
00:20:54,180 --> 00:20:56,730
我就可以计算出这个平均的延时

325
00:20:57,180 --> 00:21:01,440
就不是这种按照这种80线、95线、99线这种来分的了

326
00:21:01,560 --> 00:21:02,880
我可以再加上一条线

327
00:21:06,690 --> 00:21:11,430
第二个面板，就是qps，还有qps by instance，还有internal sql qps

328
00:21:12,690 --> 00:21:18,000
其实这里特别需要注意的一个点就是，因为qps大家都会看嘛，就是一个这个面板可能

329
00:21:18,005 --> 00:21:22,200
总的qps，这个面板是一个按照instance来分的qps

330
00:21:23,430 --> 00:21:28,740
最后这个是一个内部，internal qps，internal qps其实就是tidb内部

331
00:21:28,745 --> 00:21:34,050
因为也需要获取一些数据啊，或者是做一些更新啊，更新系统表什么的这些

332
00:21:34,230 --> 00:21:35,730
操作

333
00:21:36,060 --> 00:21:37,860
所产生的这种query

334
00:21:38,100 --> 00:21:41,190
的统计，所以最后是一个internal qps

335
00:21:44,040 --> 00:21:48,240
就需要注意的就是，如果说有异常高的internal qps的话，可能会

336
00:21:48,540 --> 00:21:52,590
第一，可能它意味着系统的一个不正常，第二就是他可能会

337
00:21:52,595 --> 00:21:56,040
导致你的这个普通的qps的值不是很准

338
00:21:56,790 --> 00:22:00,060
因为整体的qps是包含这个internal qps的

339
00:22:01,080 --> 00:22:04,350
所以这时候要注意这个internal qps对整体的影响

340
00:22:04,560 --> 00:22:09,750
如果说internale qps是比较正常啊，这时候你看性能，你就看自己这个qps就可以了

341
00:22:10,050 --> 00:22:14,460
如果不是的话，如果internale qps可以达到比如说几百，可能他就是一个

342
00:22:14,520 --> 00:22:16,650
比较异常的值了，然后这时候可能

343
00:22:16,740 --> 00:22:19,110
整体qps要进行相应的扣减

344
00:22:22,830 --> 00:22:24,720
第三个是这个connection count

345
00:22:25,020 --> 00:22:28,860
这里呢，就是由于我们有一个token limit，大家可能

346
00:22:28,950 --> 00:22:33,060
熟悉tidb的同学会知道，tidb有一个叫token limit的一个参数

347
00:22:33,990 --> 00:22:38,190
它默认的值是1000，其实这个参数后面我会介绍

348
00:22:38,370 --> 00:22:39,210
然后

349
00:22:39,510 --> 00:22:43,980
其实之所以设置这个值的原因就是不建议在一个tidb上

350
00:22:43,985 --> 00:22:49,290
创建太多连接，创建太多连接来讲就是说这时候每个连接都会争抢

351
00:22:49,295 --> 00:22:51,180
资源，其实tidb比较像

352
00:22:51,185 --> 00:22:56,490
这个能怎么说呢？跟mysql跟oracle不一样的地方，就是我们是一个无状态可以扩展的

353
00:22:56,495 --> 00:23:01,800
一个数据库，然后与其你在一个tidb创建太多连接，还不如多起一个

354
00:23:01,805 --> 00:23:04,260
tidb实例，让它把这些连接接下来

355
00:23:06,390 --> 00:23:11,700
然后下面是这个heap memory usage，一般通过这个面板看到这个，比如说

356
00:23:11,705 --> 00:23:14,220
heap有异常的变化，比如说

357
00:23:14,700 --> 00:23:19,890
heap的快速的增长，就是内存的快速增长，达到几十G或者上百G

358
00:23:20,580 --> 00:23:25,890
然后这时候可能就会出现一些问题，出现一些问题的时候，我们怎么查这些问题，可能一个就是

359
00:23:25,895 --> 00:23:26,760
我们进行pprof

360
00:23:26,910 --> 00:23:29,160
heap的一个原因吧！

361
00:23:29,550 --> 00:23:30,600
当我们发现内存

362
00:23:30,810 --> 00:23:33,840
有问题的时候，我们可能进行一些pprof heap操作

363
00:23:37,140 --> 00:23:40,080
下面是这个transaction statement num

364
00:23:41,310 --> 00:23:46,620
之前刚才讲过tidb，我们基于这种乐观事务模型，我们会把所有的

365
00:23:46,625 --> 00:23:48,600
这个语句先缓在自己内部

366
00:23:48,750 --> 00:23:51,510
然后呢，在进行这个

367
00:23:52,590 --> 00:23:53,370
最终会

368
00:23:53,375 --> 00:23:58,680
把所有的事务内写入操作在commit的时候进行一次提交，所以说不建议这个

369
00:23:58,685 --> 00:24:03,990
一个事务内的语句太多，所以这个面板就是看一个事务内语句是不是特别多

370
00:24:04,830 --> 00:24:06,660
像我这个截图里边

371
00:24:06,690 --> 00:24:09,720
单事务内语句大概只有一条

372
00:24:09,930 --> 00:24:15,240
而如果是这个面板的值非常高的话可能要注意，可能这时候对tidb本身就

373
00:24:15,300 --> 00:24:16,260
不是很友好

374
00:24:18,660 --> 00:24:20,370
下面是retry num

375
00:24:20,520 --> 00:24:25,830
retry num其实指的也就是刚才我介绍事务的时候，比如说tidb遇到写

376
00:24:25,835 --> 00:24:28,980
冲突的时候，我会进行这个事务的重试

377
00:24:29,670 --> 00:24:34,860
然后整个事务重试呢，都是包含在这个提交这一个语句之内的，所以说

378
00:24:35,640 --> 00:24:38,460
还是说如果说这个重试次数特别多的话

379
00:24:39,060 --> 00:24:43,170
相当于我一个语句执行了别人的十几个语句或几十个语句的工作

380
00:24:44,040 --> 00:24:44,790
然后

381
00:24:44,820 --> 00:24:46,500
可能就会写的会比较慢

382
00:24:46,800 --> 00:24:52,110
然后这是一个查看有没有事务重试的一个依据，当然了，它同样也是这个

383
00:24:52,115 --> 00:24:53,280
查看

384
00:24:53,490 --> 00:24:55,680
业务负载，是不是有这种

385
00:24:55,770 --> 00:24:58,290
频繁更新同一个热点的一个依据

386
00:25:01,290 --> 00:25:06,600
下面是这个kv backoff，还有 lock resolve ops

387
00:25:06,960 --> 00:25:08,580
这两个其实指的

388
00:25:08,760 --> 00:25:12,690
基本上是同一事情，也是跟上面讲的这个事务冲突有关的

389
00:25:13,290 --> 00:25:14,040
就说

390
00:25:14,045 --> 00:25:19,350
如果当事务进行冲突的时候，就是我会进行这种刚才我说等锁这种backoff操作

391
00:25:20,040 --> 00:25:25,350
这点我们介绍两个最重要的backoff吧，一个就是txnlock，这个就是代表

392
00:25:25,355 --> 00:25:28,230
我事务在写的时候，别的事务也要写

393
00:25:28,380 --> 00:25:30,090
就是我要进行bakcoff

394
00:25:30,095 --> 00:25:33,180
第二是txnlockfast

395
00:25:33,360 --> 00:25:37,890
这就是说，当这个事务正在提交的时候，遇到了就是比如别的事务

396
00:25:38,370 --> 00:25:42,030
的语句要过来读，就会造成txnlockfast

397
00:25:42,990 --> 00:25:48,300
他们也是间接表现了这个，有这种事务冲突的情况

398
00:25:49,650 --> 00:25:54,840
然后右面这个面板是lock resolve，实际上就是我说那种，就是当这个

399
00:25:54,960 --> 00:26:00,060
backoff超过它的这个最大值之后，超过它的预设的这种

400
00:26:01,230 --> 00:26:05,700
ttl之后，就是我们会进行一个解锁操作，就相当于如果

401
00:26:05,760 --> 00:26:11,070
这个事务太长时间没有提交，我们要想办法把这个没有提交事务来干掉

402
00:26:12,780 --> 00:26:15,300
然后这个会显示在这个面板里面

403
00:26:15,330 --> 00:26:16,230
说有没有？

404
00:26:16,380 --> 00:26:18,510
尝试去干掉别的事务这种事情

405
00:26:23,520 --> 00:26:25,860
下面这个面板就kv command duration

406
00:26:26,100 --> 00:26:29,760
其实这个面板对于tidb来讲是比较重要的一个面板，就是说

407
00:26:30,360 --> 00:26:34,710
它是一个区分这个问题是出现在tidb一测，还是tikv一侧的

408
00:26:35,370 --> 00:26:37,170
大家都知道，就是说

409
00:26:37,200 --> 00:26:40,260
本身这个tidb commit，其实可以做很多事情

410
00:26:40,380 --> 00:26:45,300
tidb commit本身做什么事情，就刚才我讲，比如说事务可能遇到冲突会进行重试

411
00:26:46,020 --> 00:26:50,340
包括如果你打开了binlog了之后，tidb commit也会要写binlog

412
00:26:51,000 --> 00:26:56,310
然后所以说真正的就是去kv做两阶段提交这个过程，实际上

413
00:26:56,670 --> 00:26:59,550
是整个tidb commit的一小部分

414
00:26:59,640 --> 00:27:02,670
但是这个面板实际上展示的就是

415
00:27:03,360 --> 00:27:08,010
真正去kv做操作的，每个操作的这个duration的一个平均的耗时

416
00:27:09,060 --> 00:27:14,370
然后呢，这里commit就是真正去kv进行commit的，进行两阶段提交的一个时间

417
00:27:15,120 --> 00:27:19,260
然后还有这种batch get的事件，就是真正去kv进行batch get的一个时间

418
00:27:19,380 --> 00:27:21,540
所以说，如果你发现

419
00:27:21,930 --> 00:27:25,440
就是整个tidb的这个提交时间很慢很长

420
00:27:25,950 --> 00:27:27,480
然后你再看一下这个面板

421
00:27:27,780 --> 00:27:33,090
然后就可以知道这个，如果说它也很大，执行很慢，这个

422
00:27:33,095 --> 00:27:36,330
这时候我们就知道，实际上这是因为

423
00:27:36,930 --> 00:27:39,810
可能kv的提交慢，可能tikv那边

424
00:27:40,080 --> 00:27:43,890
但如果说只是tidb自己的commit时间比较长

425
00:27:44,280 --> 00:27:48,600
但是在kv那边，其实commit的时间都很短很快

426
00:27:49,110 --> 00:27:51,750
那可能问题就还是出现tidb这侧

427
00:27:55,800 --> 00:28:01,110
下面是跟pd相关的两个面板，一个是tso wait duration

428
00:28:01,115 --> 00:28:03,150
一个是tso rpc duration

429
00:28:04,920 --> 00:28:10,230
左边这个面板的实际上代表就是我在这个获取就比如说我获取这个

430
00:28:10,235 --> 00:28:11,340
start ts

431
00:28:12,030 --> 00:28:15,540
然后到我真正使用start ts，我到底等待了多长时间？

432
00:28:15,900 --> 00:28:19,410
当然，这是一个并发的过程，有可能的一种情况就是

433
00:28:19,500 --> 00:28:22,500
tidb在获取这个start ts之后

434
00:28:22,890 --> 00:28:25,890
过了很久都没有去使用这个start ts

435
00:28:26,730 --> 00:28:32,040
也会表现为这个面板会比较高，所以呢，它有一种可能是tidb自身慢

436
00:28:32,910 --> 00:28:36,390
然后有一种可能是，另外一种可能就是获取start ts慢

437
00:28:37,710 --> 00:28:41,550
右边这个面板就是真实反映了到底获取start ts花了多长时间

438
00:28:41,610 --> 00:28:45,570
他就是彻底的就是说我start ts花了多少毫秒，就是记在这里

439
00:28:47,100 --> 00:28:51,810
所以说，这两个面板，如果是右边面板是一个较低的值，左边面板比较高的话

440
00:28:51,990 --> 00:28:54,330
这时候有慢是满在tidb自己

441
00:28:54,480 --> 00:28:59,790
就是tidb获取start ts到真正使用start ts的时候，花的时间

442
00:28:59,795 --> 00:29:00,750
比较长

443
00:29:01,260 --> 00:29:04,770
这里面可能有其他的一些原因，tidb内部自身，比如说

444
00:29:04,950 --> 00:29:10,260
处理一些优化，怎么把这个sql优化，或者等等各种事情的时候，花的时间

445
00:29:10,265 --> 00:29:11,310
会比较长

446
00:29:11,730 --> 00:29:12,630
但如果

447
00:29:13,920 --> 00:29:19,230
两个面板差不多，而且都比较高的话，可能是因为tidb跟pd之间这个网络

448
00:29:19,620 --> 00:29:20,640
延时会比较大

449
00:29:21,630 --> 00:29:26,940
好，刚才那个我们介绍一下监控的相关部分，就是说有可能这个性能问题是从

450
00:29:26,945 --> 00:29:28,080
监控上发现的

451
00:29:28,380 --> 00:29:33,180
下面呢，我们还有一个途径就是通过tidb慢日志发现性能问题

452
00:29:33,540 --> 00:29:37,020
然后我们就介绍一下tidb慢日志的相关的东西

453
00:29:37,320 --> 00:29:42,630
一部分呢其实就是想介绍tidb现在慢日志是什么样子，另外就是怎么处理这这些慢日志，包括

454
00:29:42,635 --> 00:29:47,550
这么几个工具，有这个explain explain analyze、tracing、还有admin show slow这么几个工具

455
00:29:48,900 --> 00:29:53,730
首先，先看一下我这个慢日志一个样例，现在这个慢日志大概是长这个样

456
00:29:53,735 --> 00:29:56,460
这是一个旧版的，是2.1的这个版本里

457
00:29:56,910 --> 00:29:57,870
呃

458
00:29:57,900 --> 00:30:03,210
可能比较新的版本或者将来预计发布3.0版本就会改变这个样子，这个样子可能

459
00:30:03,570 --> 00:30:05,880
看起来不是特别的好看，因为它是吧

460
00:30:05,910 --> 00:30:07,380
所有日志打成一行

461
00:30:08,790 --> 00:30:10,890
这里面就包含比如说这个

462
00:30:10,980 --> 00:30:16,290
一个标志性的一个字符串，叫slow query，还有后面是cost time

463
00:30:16,295 --> 00:30:17,820
是执行时间

464
00:30:18,390 --> 00:30:22,140
然后后面是process time，还有wait time，还有backoff time

465
00:30:22,470 --> 00:30:26,520
这些时间的实际是在tikv里并发的执行的一个执行时间

466
00:30:26,730 --> 00:30:30,960
所以说，它的时间总和，实际上可能会超过这个cost time

467
00:30:31,650 --> 00:30:34,740
然后request time代表它到底是发了request

468
00:30:35,400 --> 00:30:40,710
基本上就是对应一个region发一个request，下面是total keys，还有processed keys

469
00:30:41,250 --> 00:30:45,870
这两个值实际上代表的是什么呢？就是total keys就是我到底是这个

470
00:30:46,470 --> 00:30:47,970
扫描过多少个k

471
00:30:48,330 --> 00:30:51,930
processed keys代表我到底是这个

472
00:30:52,440 --> 00:30:53,820
到底处理了多少key

473
00:30:54,360 --> 00:30:58,380
这两个key的差别，如果说一个大一个小，差别

474
00:30:58,500 --> 00:31:03,810
会比较大的情况下，很大的可能就是我们tikv里面存在比较大的，这样的这种

475
00:31:03,930 --> 00:31:05,790
旧的数据没有被删除

476
00:31:06,030 --> 00:31:07,200
就是过期数据

477
00:31:07,650 --> 00:31:12,960
当然我们这个数据gc比较正常的情况下，我们是不会有任何过期数据的

478
00:31:13,620 --> 00:31:16,350
然后所以这时候可能这两个key这个

479
00:31:16,800 --> 00:31:18,360
总数会差的比较小

480
00:31:20,100 --> 00:31:23,460
然后后面这个标志succ就标志这个query执行结果

481
00:31:24,960 --> 00:31:29,790
包括一些connection id啊，然后user信息啊，还有这个

482
00:31:30,000 --> 00:31:32,730
事务的一个start ts，还有的一些

483
00:31:33,060 --> 00:31:36,000
最终会包含这个事务的sql，长什么样子

484
00:31:36,660 --> 00:31:39,660
这还是一些比较详细的一些内容

485
00:31:40,200 --> 00:31:42,960
然后新版的这个慢日志就变成了这个样子

486
00:31:43,560 --> 00:31:45,120
呃

487
00:31:45,240 --> 00:31:50,550
就其他程度上，这个模仿mysql本身慢日志的样子，然后更便于

488
00:31:50,555 --> 00:31:52,770
大家用一些工具进行解析

489
00:31:53,160 --> 00:31:56,910
所包含这些内容，其实还是这些

490
00:31:56,970 --> 00:31:58,260
差不多的内容

491
00:31:59,880 --> 00:32:05,190
然后当我拿到这个慢日志内容，看到一个sql执行很慢的时候，我们怎么办呢

492
00:32:05,195 --> 00:32:07,920
第一步就是我们用这种explain来执行，

493
00:32:08,610 --> 00:32:10,530
看一下它的执行计划是不是正常

494
00:32:11,370 --> 00:32:16,680
然后explain得到结果就是这个sql具体的执行计划，像这个里面这个例子就是这个tidb的执行计划

495
00:32:17,130 --> 00:32:20,820
然后第一步包含他的这个到底有几个算子

496
00:32:21,090 --> 00:32:24,240
都是什么算子，然后几个算子之间的一个关系

497
00:32:24,960 --> 00:32:28,020
然后是从谁到谁，谁到谁？

498
00:32:28,230 --> 00:32:33,540
然后下面是第二列代表它的count，也就是说他是一个统计信息估计的值，到底每一个

499
00:32:33,545 --> 00:32:36,630
算子，到底要处理多少个k

500
00:32:36,840 --> 00:32:37,860
多少行数据

501
00:32:38,370 --> 00:32:43,680
然后第三个task代表，它到底是在tidb计算的一个root task，还是下推下去的

502
00:32:43,685 --> 00:32:47,880
一个coprocess task，就是在tikv里执行的

503
00:32:48,300 --> 00:32:53,610
最后一个部分就是每一个算子自己执行的一个，更多

504
00:32:53,615 --> 00:32:58,590
的细节的信息吧，就把它第一个算子，包括第一个算子是tidb operation，它是一个算子

505
00:32:58,860 --> 00:33:00,960
实际它做的是一个count的操作

506
00:33:01,590 --> 00:33:02,760
然后呢下面

507
00:33:02,850 --> 00:33:07,410
在这个地方，实际还是有count，它count的是

508
00:33:07,530 --> 00:33:12,480
所有的值，然后上面count是什么，其中某一列的值，它跟这个case有关

509
00:33:14,220 --> 00:33:18,720
最终下面是到底我这个扫多少数据，范围是什么，region从哪到哪儿

510
00:33:19,380 --> 00:33:24,690
所以可以看这个得到一个更详细的一个执行计划是怎么什么样子，但是

511
00:33:25,080 --> 00:33:27,420
OK，后面我们又

512
00:33:27,540 --> 00:33:29,520
呃，扩展了这个语法

513
00:33:29,850 --> 00:33:35,160
扩展了语法就是我们从postgresql去学到了explain analyze，explain analyze

514
00:33:35,165 --> 00:33:38,670
跟explain的区别是什么呢，就是上面这个explain实际上

515
00:33:38,790 --> 00:33:40,290
他是把这个

516
00:33:40,350 --> 00:33:43,380
sql具体要怎么执行告诉我们

517
00:33:43,830 --> 00:33:47,820
但是下面这个sql实际上告诉我们的是这个sql到底是

518
00:33:48,000 --> 00:33:50,490
呃，实际执行起来是什么样子的

519
00:33:51,300 --> 00:33:52,410
而不是一个它

520
00:33:52,620 --> 00:33:56,640
将要说是怎么执行，也就是说它是一个执行后结果的一个统计

521
00:33:57,780 --> 00:34:03,090
我们看这个例子，这个例子里面就是explain analyze一个这样sql，这个sql实际上是

522
00:34:03,095 --> 00:34:04,410
tpc-h的一个sql

523
00:34:04,950 --> 00:34:08,670
然后呢，左边同样它是包含了这个sql的这个

524
00:34:08,820 --> 00:34:11,910
每个算子之间一个逻辑关系，一个

525
00:34:12,300 --> 00:34:13,170
plan tree

526
00:34:13,440 --> 00:34:18,750
然后右边的实际上，当然这也是一个截图啊，就是说

527
00:34:18,755 --> 00:34:20,490
其实它也是包含这几列的

528
00:34:20,640 --> 00:34:24,570
但是他除了自己的之前，额外多了一列，就是execution info

529
00:34:25,350 --> 00:34:30,660
然后它是告诉大家就是每个算子具体的执行时间是多少，实际执行时间，不是

530
00:34:30,960 --> 00:34:33,660
一个估计的的什么之类的，然后是具体的这个

531
00:34:33,810 --> 00:34:35,130
执行次数，然后

532
00:34:35,550 --> 00:34:37,860
具体处理多少行数据

533
00:34:37,920 --> 00:34:39,060
都在这里显示

534
00:34:39,690 --> 00:34:41,220
所以说，如果说

535
00:34:41,370 --> 00:34:42,480
呃

536
00:34:43,230 --> 00:34:44,400
我们的这个

537
00:34:44,640 --> 00:34:46,920
呃，sql条件允许的话

538
00:34:47,280 --> 00:34:50,550
能使用explain analyze，就要使用explain analyze

539
00:34:50,940 --> 00:34:56,250
但是如果说这个sql执行的时间太长，或者说的这个sql如果真正执行下去的话，就是

540
00:34:56,550 --> 00:34:57,990
它会发生什么异常

541
00:34:58,260 --> 00:35:01,890
只能使用explain，那我们还是用原来的explain来进行

542
00:35:02,040 --> 00:35:03,450
排查，看这个

543
00:35:03,480 --> 00:35:05,460
sql本身是不是有什么问题

544
00:35:08,040 --> 00:35:12,060
下面这个工具是tracing，这是我们一个新功能

545
00:35:12,540 --> 00:35:15,840
它用法也很简单，其实跟explain比较类似

546
00:35:16,020 --> 00:35:21,330
explain是这个我直接这么一个explain，然后再加我们explain analyze，然后加我们的sql

547
00:35:21,810 --> 00:35:22,770
tracing呢是

548
00:35:22,920 --> 00:35:25,140
用trace加我们的sql

549
00:35:25,590 --> 00:35:30,180
然后tidb自身，大家都知道在这个状态报端口10080

550
00:35:30,420 --> 00:35:35,550
10080端口打开之后，有web trace实际上是可以有一个图像化界面

551
00:35:35,940 --> 00:35:41,250
这个界面是怎么用的，实际上，就是我们把这个trace的这个结果可以拷贝出来

552
00:35:41,700 --> 00:35:42,390
然后

553
00:35:42,510 --> 00:35:43,860
然后再打开这个

554
00:35:44,010 --> 00:35:47,370
和web界面，然后就是我们可以把这个trace结果放进去

555
00:35:48,210 --> 00:35:53,010
这里得到的一个是，其实就是图形化展示，他会告诉你这个sql

556
00:35:53,340 --> 00:35:58,650
就不像上面explain那样了，他explain本身是一种以算子为

557
00:35:58,655 --> 00:36:00,420
为粒度进行统计的

558
00:36:00,660 --> 00:36:05,970
但是这个trace它是以这种函数为粒度进行统计的，她会统计每个函数实际执行时间

559
00:36:07,050 --> 00:36:12,360
然后包括其实这个也可以看到在整个execute这个函数里边，它是

560
00:36:12,365 --> 00:36:14,490
包含前面是这几个函数的

561
00:36:16,260 --> 00:36:19,230
然后每个函数执行时间都会比较详细的打在上面

562
00:36:19,260 --> 00:36:21,780
这是一个相对比较高级点的功能吧

563
00:36:24,870 --> 00:36:28,140
下面是admin show slow，就是说

564
00:36:28,770 --> 00:36:34,080
有很多用户跟我们抱怨，就是说，比如我们这个slow query还是不太好查，所以后来我们就做

565
00:36:34,085 --> 00:36:38,670
这么一个工具，这是一个不是查慢日志有什么问题的工具，而是

566
00:36:38,880 --> 00:36:41,790
一个查有哪些慢日志的一个工具

567
00:36:42,420 --> 00:36:45,030
就是说我们可以用这样一条sql去把

568
00:36:45,060 --> 00:36:50,370
最近七天里边最多的500条的这个日志给打出来

569
00:36:50,700 --> 00:36:56,010
比如recent 10就是最近的10条，当前也可以用top命令，如果top 10

570
00:36:56,015 --> 00:36:57,720
的话就是最多这个

571
00:36:57,960 --> 00:37:00,630
呃，top30就是最慢的这个

572
00:37:00,780 --> 00:37:03,150
慢日志是哪些

573
00:37:03,450 --> 00:37:04,830
和一个详细的列表

574
00:37:07,200 --> 00:37:12,360
呃，第四部分我介绍一下，就是说性能调优的这些一些参数是

575
00:37:12,690 --> 00:37:17,880
怎么调优的？然后其实分为了两部分，一个是配置参数部分，一个是系统变量部分

576
00:37:19,410 --> 00:37:24,720
配置参数呢，不多，然后这些都是存在tidb配置文件中的

577
00:37:25,050 --> 00:37:30,360
然后系统参数就是这个，我们通过sql可以在线进行调整的，我可以用sql，比如set命令

578
00:37:30,365 --> 00:37:31,020
进行set

579
00:37:33,600 --> 00:37:38,910
先看第一个，token limit，token limit刚才在介绍那个connection监控的时候

580
00:37:38,915 --> 00:37:39,930
介绍过

581
00:37:40,140 --> 00:37:45,000
提到过这个地方，就是说token limit限制了正在工作的session的数据

582
00:37:45,300 --> 00:37:46,830
它默认值是1000

583
00:37:47,040 --> 00:37:51,840
并发数超过这个值的时候呢，实际上就会发生等待的现象，就是说如果说

584
00:37:52,050 --> 00:37:54,420
呃，当然我可能同时能创建，比如说

585
00:37:54,810 --> 00:37:57,870
超过1000个连接，但是呢？

586
00:37:57,990 --> 00:38:02,220
真正在执行sql的这个连接只有这个1000个

587
00:38:03,090 --> 00:38:06,630
对于这个OLAP引用，其实他有很好的这个防误触作用的

588
00:38:08,010 --> 00:38:13,320
曾经有一个用户，就是他在设计一个OLAP的一个场景的时候，他在自己应用端

589
00:38:13,325 --> 00:38:14,580
很可能就是

590
00:38:14,610 --> 00:38:17,010
并没有设置很好的这种防误触的作用

591
00:38:17,015 --> 00:38:22,320
然后他的那个报表生成那个按钮，他可以连续点击，然后每次点击都会生成一个很

592
00:38:22,650 --> 00:38:25,590
复杂的sql，复杂sql就会发到数据库这端

593
00:38:26,160 --> 00:38:27,210
然后因为

594
00:38:27,240 --> 00:38:32,310
大家都知道，这种olap应用这种，这个sql返回的时间是比较长的，然后他可能

595
00:38:33,330 --> 00:38:38,640
用户使用的时候并没有这个意识，又不知道这个按一下这个按钮会带来什么影响，也不

596
00:38:38,645 --> 00:38:40,620
这个sql什么时候会返回

597
00:38:40,770 --> 00:38:41,640
他会

598
00:38:41,730 --> 00:38:45,390
反复点击，然后之后就会把数据库那边儿造成比较大压力

599
00:38:45,660 --> 00:38:47,040
我这时候其实我们

600
00:38:47,130 --> 00:38:51,810
做一个反向的优化，然后就是相当于把这个这个参数调整为1

601
00:38:51,930 --> 00:38:54,420
然后让同时执行的sql数只有一个

602
00:38:55,020 --> 00:38:58,620
这样就是即使对方再怎么连续点击其实

603
00:38:59,640 --> 00:39:01,470
执行这个sql的这个

604
00:39:01,530 --> 00:39:03,090
连接其实只有一个连接

605
00:39:06,000 --> 00:39:11,310
额，第二个参数就是在tidb配置一个max-procs的参数，max-procs实际上

606
00:39:11,315 --> 00:39:14,850
限制了server最多使用的CPU核数

607
00:39:15,720 --> 00:39:16,680
呃

608
00:39:17,250 --> 00:39:21,840
这个其实本身其实就单独使用的话，可能感觉用处不是特别大，但是

609
00:39:21,845 --> 00:39:27,150
如果说我们就用它来配合这个taskset，taskset是linux的一个命令，然后可以

610
00:39:27,155 --> 00:39:31,770
充分利用，比如说像新的numa架构的这种物理资源

611
00:39:32,490 --> 00:39:35,070
下图就是我们一个实验，就是我们

612
00:39:35,790 --> 00:39:41,100
在同样一个物理机上，这个物理机是有两个numa CPU的，然后呢每个numa

613
00:39:41,105 --> 00:39:42,510
CPU大概有20核

614
00:39:42,870 --> 00:39:46,140
然后我们启动两个tidb，然后同时给他们

615
00:39:46,950 --> 00:39:52,260
每个tidb 500并发，然后和启动一个tidb，一个tidb呢，然后给它

616
00:39:52,500 --> 00:39:54,900
让它用满这40核，给它1000并发

617
00:39:55,290 --> 00:40:00,600
其实我们就会发现，其实两个tidb均摊了这个压力，然后呢？并且他们通过

618
00:40:00,605 --> 00:40:05,910
taskset绑核之后，可以让他们这个CPU访问内存不会跨numa节点，然后这样的

619
00:40:05,915 --> 00:40:08,400
这个整个CPU的使用率是最高的

620
00:40:08,730 --> 00:40:10,230
但如果我们把这个

621
00:40:10,380 --> 00:40:11,220
呃

622
00:40:11,640 --> 00:40:16,920
tidb自由地去用40核的话，可能就会发生一些比如说跨numa核心的一个问题

623
00:40:17,190 --> 00:40:21,210
然后它整个CPU利用率也不会特别大，也不会去完全打满

624
00:40:23,970 --> 00:40:29,280
下面要讲的这个prepared-plan-cache，对于这个execute语句来讲，其实呢？

625
00:40:29,550 --> 00:40:32,820
他可以缓存第一次execute的执行计划

626
00:40:33,960 --> 00:40:34,740
然后

627
00:40:34,830 --> 00:40:38,790
呃，就是说意思我们prepare一个语句之后，这时候我们可能

628
00:40:38,910 --> 00:40:41,340
在执行execute的时候，只会传几个参数过来

629
00:40:41,490 --> 00:40:46,500
然后呢，传几个参数过来，其实我们可以通过一次execute，把这个参数的得到之后，然后以及

630
00:40:46,830 --> 00:40:48,750
传给我们这个嗯

631
00:40:49,050 --> 00:40:51,450
语句的id，就会把整个语句拼出来

632
00:40:51,660 --> 00:40:53,460
但就是这样，就是其实

633
00:40:53,490 --> 00:40:55,710
prepare这样的默认的一个做法嘛

634
00:40:55,980 --> 00:40:57,960
它实际上是缓存了这个sql

635
00:40:58,410 --> 00:41:01,560
然后这样避免了这个sql的一个解析的过程，但是

636
00:41:01,590 --> 00:41:03,120
我们更进一步，我们

637
00:41:03,240 --> 00:41:06,660
可以把第一次execute执行的执行计划缓存下来

638
00:41:07,260 --> 00:41:12,570
缓存这个执行计划之后，我们不仅可以把语句拼出来，当这个第二次execute的时候，也可以

639
00:41:12,575 --> 00:41:13,560
可以把这个

640
00:41:13,740 --> 00:41:18,390
plan就不用让他重新做build的，我让它拿出来可以直接使用

641
00:41:18,900 --> 00:41:23,880
然后对于这种比如说比较严苛，这种对性能要求比较高的oltp场景，其实

642
00:41:24,030 --> 00:41:26,400
发现可以大概提升17%的性能

643
00:41:26,850 --> 00:41:30,690
但是这也需要注意的是，它自身是有一个容量限制的就是说

644
00:41:30,840 --> 00:41:35,880
呃，我最多可以缓存多少个，这个语句的执行计划

645
00:41:36,270 --> 00:41:41,400
然后它是一个session级的变量，就是说如果说一个session就缓存比如说100个200个

646
00:41:41,700 --> 00:41:47,010
然后你如果启动1000个session的话，可能整个需要缓存的执行计划就会特别多

647
00:41:47,490 --> 00:41:49,260
然后所以这时候可能

648
00:41:49,410 --> 00:41:52,140
内存使用方面可能需要注意一下

649
00:41:55,110 --> 00:41:57,810
然后下一个是txn-local-latches

650
00:41:57,960 --> 00:42:03,270
呃，这个参数实际上就是tidb server内部呢，是它事先

651
00:42:03,275 --> 00:42:05,640
对事务的提交顺序进行了排序

652
00:42:06,150 --> 00:42:10,680
然后这样有什么好处，就是可以保证，如果从我这个sever发出去的commit

653
00:42:11,430 --> 00:42:15,570
请求不会跟我这个server自己commit请求不会发生冲突

654
00:42:16,890 --> 00:42:20,010
然后呢，对于一些高并发，高冲突的这种场景

655
00:42:20,580 --> 00:42:25,890
这是非常有利的，但是如果说你的这个场景是要经常有这种跨

656
00:42:25,895 --> 00:42:26,940
tidb的这种

657
00:42:27,000 --> 00:42:32,310
事务，还有高冲突的话，它的作用就不会这么明显了，但是如果说

658
00:42:32,760 --> 00:42:33,780
这个冲突

659
00:42:33,810 --> 00:42:35,100
可以把它约束在

660
00:42:35,105 --> 00:42:37,710
所有的事务冲突都发生在一个tidb之内，他就

661
00:42:38,340 --> 00:42:43,650
这样他可以预先进行一次排队，然后就是如果是已经发生了这种不可避免

662
00:42:43,920 --> 00:42:47,190
的冲突的话，也会提前会把这个错误报给你报出来

663
00:42:48,150 --> 00:42:53,460
但是呢，它有一个限制，就是说如果是读写的这种冲突，其实还是会backoff的，因为tidb并没有缓存

664
00:42:53,700 --> 00:42:55,440
所有的这种已读的数据

665
00:42:55,770 --> 00:42:57,120
只缓存了这个

666
00:42:57,270 --> 00:42:58,560
我需要写的数据

667
00:43:01,260 --> 00:43:03,210
下面讲的是系统变量的部分

668
00:43:03,630 --> 00:43:08,940
系统变量部分呢，首先先介绍一个跟这个mysql兼容的一个insert行为的一个参数

669
00:43:10,050 --> 00:43:14,430
呃，这参数就是跟我们刚才介绍的，就是说我说mysql

670
00:43:14,820 --> 00:43:16,560
tidb跟mysql有很多

671
00:43:16,565 --> 00:43:20,400
不同，比如其中一个就是写入不同，tidb做了lazy check

672
00:43:20,790 --> 00:43:23,850
就是在这个提交时候才检查，但是这个参数呢？

673
00:43:24,120 --> 00:43:26,490
它作用就是如果我们把它打开的话

674
00:43:26,730 --> 00:43:30,990
实际上，它是会让tidb跟mysql行为是完全一致的

675
00:43:32,280 --> 00:43:34,560
如果我们开启了这个这个变量的话

676
00:43:34,830 --> 00:43:35,760
他就会

677
00:43:35,850 --> 00:43:40,620
像右边这个例子一样，就是mysql在这里报错，tidb也会在这里报错

678
00:43:41,220 --> 00:43:46,530
然后呢，mysql在这里提交成功，tidb也会提交成功，然后最终结果也是完全一致的

679
00:43:47,220 --> 00:43:48,120
但是这里

680
00:43:48,630 --> 00:43:51,360
任何事情都是有一个这个取舍的

681
00:43:51,510 --> 00:43:56,820
如果我们开启了这个东西，它会造成什么问题呢？就是我们如果一个事务里面的语句

682
00:43:56,825 --> 00:43:59,130
特别多的话，我每一个begin

683
00:43:59,670 --> 00:44:04,110
到commit之间，比如说1000个2000个insert

684
00:44:04,530 --> 00:44:08,010
每个insert的每一行都要去进行这个

685
00:44:08,040 --> 00:44:11,550
检查，检查是不是有唯一的这个约束的冲突

686
00:44:12,300 --> 00:44:17,340
如果说已经发生了的话，那我就当时报错了，但是如果没有的话，我就去检查了

687
00:44:17,730 --> 00:44:23,040
所以说，很有可能整个这个事务期间一个冲突都没有，但是你做了n多次检查

688
00:44:23,045 --> 00:44:25,350
而没有做在提交时做一次检查

689
00:44:25,650 --> 00:44:30,180
这样对其实对这种这种事务的话，对性能影响是比较大的

690
00:44:31,530 --> 00:44:35,700
大多数情况下，不建议大家开启这个参数，什么时候适合开启呢，就是

691
00:44:35,940 --> 00:44:36,810
如果

692
00:44:37,980 --> 00:44:41,100
在使用一些比如说像ogg这种同步工具的时候

693
00:44:41,670 --> 00:44:46,980
他是会需要检查这个这个结果的，他会把这个自己这个

694
00:44:46,985 --> 00:44:49,170
因为我们是一个，呃

695
00:44:49,200 --> 00:44:51,240
兼容mysql协议的一个数据库嘛

696
00:44:51,540 --> 00:44:56,010
因为他会认为tidb就是一个mysql，认为你需要跟mysql完全一致的行为

697
00:44:56,670 --> 00:44:57,870
所以他这时候会

698
00:44:57,960 --> 00:45:03,270
呃，在这种insert报错时候它也会期望tidb也需要报错，但tidb并没有

699
00:45:03,840 --> 00:45:09,150
所以说，只有在使用特殊工具的时候，可能我们需要打开这个参数，大多数情况其实都不需要打开这个参数

700
00:45:10,260 --> 00:45:15,120
当然然后就如果发现这个性能有问题的话，那我们还是可能需要开一下这个参数

701
00:45:16,620 --> 00:45:19,350
看一下这个参数是不是被错误的打开了

702
00:45:20,760 --> 00:45:24,390
下面看一下distsql_scan_concurrency

703
00:45:24,900 --> 00:45:29,430
这个参数呢，实际上是控制的tidb向tikv发的这个扫描数据一个并发数

704
00:45:30,120 --> 00:45:35,430
然后呢，默认值就是它默认的要向多少个region发送并发的请求

705
00:45:35,910 --> 00:45:40,740
这个参数实际上是一个作用范围比较大的，它实际上对于数据和索引都是有效的

706
00:45:42,600 --> 00:45:47,910
然后其他这些参数呢，就是我统一列到这里，他们都是对于这个单独算子进行

707
00:45:47,915 --> 00:45:49,230
一些

708
00:45:49,290 --> 00:45:52,110
小范围的一个调优的参数，比如说第一个

709
00:45:52,350 --> 00:45:56,610
呃，hashagg_partial_concurrency就是hash聚合的一个并发数

710
00:45:56,820 --> 00:45:58,800
然后hash join的一个并发数

711
00:45:59,100 --> 00:46:04,410
然后下面是index lookup join和index join batch size

712
00:46:05,370 --> 00:46:10,680
实际上，它控制的就是index lookup，也就是我们index lookup join的并发数和每次batch的大小

713
00:46:11,370 --> 00:46:16,680
然后下面是index lookup size还有index lookup concurrency，就是index lookup就是

714
00:46:17,160 --> 00:46:19,680
所谓的先读索引再读数据

715
00:46:19,800 --> 00:46:24,870
相当于double read的这种过程，或者叫回表中过程的一个并发数，和这个batch size 

716
00:46:25,740 --> 00:46:31,050
最后一个参数，它是一个布尔值，它是一个就相当于控制到底是把这个子查询

717
00:46:31,260 --> 00:46:34,920
展开为join啊

718
00:46:35,220 --> 00:46:38,310
还是把in里的子查询，按照in里的子查询

719
00:46:38,400 --> 00:46:40,440
自己执行完了之后再拿出来用

720
00:46:41,400 --> 00:46:42,450
一个开关

721
00:46:42,840 --> 00:46:48,150
在这个如果in里的子查询执行比较快，数据量比较小的时候，实际上是把它在

722
00:46:48,450 --> 00:46:53,250
当成in用来是比较好的，就是让他作为一个子查询来执行就行了

723
00:46:53,550 --> 00:46:58,860
如果说它in里子查询结果集比较大，执行时间也比较长，那把它拿

724
00:46:58,865 --> 00:47:01,410
出来做join可能是一个更好的选择

725
00:47:04,170 --> 00:47:09,480
下面就是我们这个答疑安排，就是可能希望大家登录这个pingcap的网校我们进行答疑

726
00:47:10,830 --> 00:47:13,080
我今天就讲了就是这些

727
00:47:17,790 --> 00:47:18,390
谢谢大家
