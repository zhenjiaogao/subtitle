1
00:00:00,030 --> 00:00:05,340


2
00:00:13,320 --> 00:00:18,630
PingCAP University 的同学们，大家好，今天我们要讲的是 TiDB 的备份

3
00:00:18,635 --> 00:00:19,620
与恢复

4
00:00:20,640 --> 00:00:25,950
然后我自我介绍一下我叫周跃跃，PingCAP生态技术工程师

5
00:00:29,610 --> 00:00:34,050
今天我们讲的主要内容是TiDB数据库的备份与恢复

6
00:00:34,230 --> 00:00:39,540
通过这节课的学习，我们要了解TiDB的备份原理与数据

7
00:00:40,170 --> 00:00:41,940
备份恢复的一些方案

8
00:00:43,080 --> 00:00:48,120
然后主要知识点包括四方面，第一块是TiDB-Binlog这个工具的介绍

9
00:00:48,210 --> 00:00:51,300
第二个方面是TiDB Binlog的备份原理

10
00:00:51,570 --> 00:00:54,060
第三块是备份工具的恢复

11
00:00:54,240 --> 00:00:56,670
第四块是一些恢复案例的介绍
第三块是备份工具的恢复
12
00:01:01,380 --> 00:01:01,980
第四块是一些恢复案例的介绍

13
00:01:04,230 --> 00:01:07,890
首先我们看一下TiDB-Binlog版本的

14
00:01:08,850 --> 00:01:13,470
发布，首先我们第一个版本是2017年的rc版本

15
00:01:13,530 --> 00:01:18,840
第二个版本是18年我们发布的1.0版本，然后现在我们经常用到的就是

16
00:01:18,845 --> 00:01:19,680
我们

17
00:01:19,830 --> 00:01:24,270
18年下半年发布的2.0版本，也是最常用的版本

18
00:01:26,250 --> 00:01:29,760
这个是我们rc版本的架构图

19
00:01:31,290 --> 00:01:33,780
这个是我们1.0版本的

20
00:01:33,900 --> 00:01:39,210
架构图，大家可以看一下这两个版本，因为我们现在今天要讲的主要是

21
00:01:39,240 --> 00:01:44,550
我们2.0版本也就是TiDB-Binlog Cluster架构，所以我们会重点讲

22
00:01:45,210 --> 00:01:46,500
这个架构

23
00:01:47,160 --> 00:01:52,470
首先我们看一下这个图，我们可以看到TiDB-Binlog的上游是接

24
00:01:52,475 --> 00:01:55,080
了一个TiDB的一个服务

25
00:01:55,170 --> 00:01:56,610


26
00:01:56,820 --> 00:02:02,130
所以对TiDB的写入会产生binlog，通过Pump去拉下来，之后

27
00:02:02,135 --> 00:02:03,120
然后去

28
00:02:03,150 --> 00:02:07,980
发送到Drainer，由Drainer，再同步到下游，比如说下游可以是一个MySQL

29
00:02:08,010 --> 00:02:09,450
可以是一个TiDB Server

30
00:02:09,455 --> 00:02:11,520
以及其它的等等

31
00:02:16,230 --> 00:02:21,540
前面我们介绍了TiDB-Binlog的一个架构，然后我们现在重点介绍

32
00:02:21,545 --> 00:02:24,000
一下TiDB-Binlog的作用

33
00:02:24,360 --> 00:02:28,860
首先，TiDB-Binlog是一个用于收集TiDB binlog的一个工具

34
00:02:29,700 --> 00:02:35,010
我们用拿到的binlog可以去做一个准实时的备份和同步功能

35
00:02:36,450 --> 00:02:41,760
所以我们可以看到，TiDB Binlog的主要有两个主要的作用，第一个是做数据的同

36
00:02:41,765 --> 00:02:42,540
步

37
00:02:42,545 --> 00:02:47,760
我们可以去同步上游的TiDB集群的binlog到下游的其它数据库

38
00:02:48,240 --> 00:02:51,210
第二个是我们可以做一个实时的备份和恢复

39
00:02:51,240 --> 00:02:55,920
因为我们已经拿到了上游的binlog，我们就相当于有了一份多余的数据

40
00:02:56,010 --> 00:03:01,320
所以这样，这个时候我们可以去做一个TiDB集群的一个数据的备份，同时

41
00:03:01,325 --> 00:03:04,230
如果在做异常情况下，比如出现上游的

42
00:03:05,280 --> 00:03:09,600
节点异常时，我们可以用来做一个数据的一个恢复

43
00:03:10,680 --> 00:03:15,360
我们通过刚才的2.0的架构图可以看到

44
00:03:16,260 --> 00:03:21,000
TiDB-Binlog，主要有两大组件组成，Pump和Drainer

45
00:03:23,310 --> 00:03:28,140
然后我们看一下Pump的功能，Pump主要是提供写binlog的服务

46
00:03:28,860 --> 00:03:30,450
同时，它会获取

47
00:03:32,160 --> 00:03:36,150
上游的binlog，提供给下游的drainer

48
00:03:37,290 --> 00:03:42,600
同时，它会做一个binlog的拼装，因为我们写binlog也是一个

49
00:03:42,605 --> 00:03:43,770
2PC的阶段

50
00:03:43,800 --> 00:03:48,090
包括一个prepare写（prewrite）和一个commit的阶段

51
00:03:48,120 --> 00:03:53,430
所以Pump的作用，它就会是把同一条事务的一个两阶段提交

52
00:03:53,670 --> 00:03:54,300
就

53
00:03:54,360 --> 00:03:55,650
拼装到一起

54
00:03:56,550 --> 00:03:59,010
第四点就是这个binlog

55
00:03:59,070 --> 00:04:04,380
因为pump会是一个多Pump的去写，所以它会需要把

56
00:04:05,100 --> 00:04:08,490
在某一个pump出现异常的时候或者出现

57
00:04:08,670 --> 00:04:10,170
写入

58
00:04:12,660 --> 00:04:17,970
写入不够及时的时候，我们会去伪造一个binlog去促进同步进度

59
00:04:21,450 --> 00:04:23,280
第二个组件是drainer

60
00:04:23,550 --> 00:04:28,860
drainer的作用是因为我们刚才可以看到在pump其实它是可以组成pump cluster的

61
00:04:29,610 --> 00:04:34,920
所以drainer的作用就是它会收集到各个pump的binlog，把binlog拿过来之后

62
00:04:34,925 --> 00:04:37,380
做一个归并排序，然后发送到下游

63
00:04:38,730 --> 00:04:41,640
drainer的第二个作用是它会做一个

64
00:04:41,940 --> 00:04:46,800
db级别和table级别的过滤，也就是说我拿到所有的

65
00:04:47,430 --> 00:04:52,740
TiDB集群里的binlog之后，我根据我自己的需要去做一个我需要同步的表

66
00:04:52,920 --> 00:04:54,780
或者是db级（schema）的一个过滤

67
00:04:57,180 --> 00:05:00,180
TiDB-Binlog工具里面还有一个组件叫

68
00:05:00,390 --> 00:05:05,700
binlog-ctl，我们刚才在架构图里没有看到这部分，因为这是一个

69
00:05:05,705 --> 00:05:11,010
类似一个集群工具管理的一个组件，它主要作用是

70
00:05:11,015 --> 00:05:13,710
获取 TiDB 集群当前的 TSO

71
00:05:13,800 --> 00:05:17,550
以及管理和查看当前pump/Drainer的作用

72
00:05:17,970 --> 00:05:18,810
状态

73
00:05:23,430 --> 00:05:28,740
前面我们大概介绍了一下，TiDB-Binlog这个工具的作用以及它的

74
00:05:28,745 --> 00:05:30,030
三个组件的

75
00:05:30,330 --> 00:05:35,640
内容，然后我们现在介绍一下TiDB binlog和MySQL binlog的一些区别

76
00:05:37,440 --> 00:05:42,750
作为分布式数据库的代表，TiDB binlog它的实现跟MySQL binlog

77
00:05:42,755 --> 00:05:44,790
的实现其实是有区别的

78
00:05:45,090 --> 00:05:50,400
我们下面主要从五个方面去介绍它的不同一个

79
00:05:50,405 --> 00:05:51,060
是
下面我们主要从 5 个方面介绍它的不同，一个是
80
00:05:51,420 --> 00:05:53,700
什么时候，我们去写binlog

81
00:05:53,760 --> 00:05:55,320
我们去往哪里写？

82
00:05:55,380 --> 00:06:00,690
我们写到的binlog存在哪里？以及我们把拿到的binlog怎么同步到

83
00:06:00,695 --> 00:06:01,590
下游

84
00:06:02,280 --> 00:06:06,720
最后一个是把已经应用过的binlog，怎么去做一个清理

85
00:06:09,660 --> 00:06:12,960
首先是第一块，怎么去写binlog？

86
00:06:13,020 --> 00:06:18,330
在写 binlog 的时候，我们刚才也介绍了，其实也跟 TiDB 的事务也是一样的，是

87
00:06:18,335 --> 00:06:20,190
一个两阶段提交的过程

88
00:06:21,420 --> 00:06:25,200
所以它写的时候它会去，并行地去写TiKV和Pump

89
00:06:25,320 --> 00:06:30,630
在这个过程中，只要有一个写失败，比如说我TiKV写失败或者是Pump写失败，

90
00:06:30,635 --> 00:06:32,340
那这个事务它就会失败

91
00:06:32,430 --> 00:06:34,320
所以进行事务的回滚

92
00:06:35,790 --> 00:06:41,100
在commit的阶段，commit阶段的时候，它是有一个先后顺序，它会先

93
00:06:41,730 --> 00:06:44,880
commit TiKV，然后再是Pump

94
00:06:45,450 --> 00:06:47,610
在这里面又会有两种

95
00:06:48,390 --> 00:06:52,080
语句的区别：DML写和DDL写

96
00:06:52,230 --> 00:06:53,760
DML写的时候

97
00:06:53,850 --> 00:06:58,950
在prepare write的时候会我们去写一条实际的包含数据的变更

98
00:06:59,040 --> 00:07:04,290
然后在数据提交，在事务的提交的时候，我们再去写一个commit binlog

99
00:07:04,740 --> 00:07:10,050
如果这个时候事务写失败了，我就会进行回滚，然后再去写一条

100
00:07:10,110 --> 00:07:11,400
rollback的binlog

101
00:07:12,750 --> 00:07:14,850
DDL写比较简单

102
00:07:15,060 --> 00:07:20,070
主要是事务，一个是我做完一个 job，或者是一个 job rollback

103
00:07:20,340 --> 00:07:23,640
之后，才会去写binlog

104
00:07:27,300 --> 00:07:32,280
刚才我们介绍的怎么写，现在我们介绍去往哪里写

105
00:07:32,700 --> 00:07:34,470
在TiDB binlog中

106
00:07:34,560 --> 00:07:37,260
TiDB是写到Pump中的

107
00:07:37,290 --> 00:07:40,590
也就是我刚才看到的架构图里面的Pump Cluster的部分

108
00:07:43,020 --> 00:07:47,940
Pump里面，它还提供了写binlog以及存储binlog

109
00:07:48,660 --> 00:07:51,750
为下游的drainer提供消费的服务

110
00:07:55,230 --> 00:07:58,740
在最开始的版本里面，我们会看到，就是下面这个图

111
00:07:58,800 --> 00:08:01,530
我们的一个TiDB Server其实对应一个Pump

112
00:08:01,560 --> 00:08:05,490
这样的话，如果我集群里面有多个TiDB Server的话我就可

113
00:08:05,520 --> 00:08:10,830
能需要多个Pump一一对应，但是在我们后来的版本中，我们做了一个优化

114
00:08:10,835 --> 00:08:12,270
就是

115
00:08:12,570 --> 00:08:14,340
Pump是有多个Pump

116
00:08:14,640 --> 00:08:19,950
然后TiDB Server也可以有多个，但是这个时候TiDB Server写的时候我不会去指定

117
00:08:20,340 --> 00:08:25,650
我去写某一个Pump，而是我TiDB Server只管去往Pump Cluster

118
00:08:25,655 --> 00:08:30,960
里面去写，但是具体写到哪一个Pump，它其实是不需要我们去指定的

119
00:08:33,330 --> 00:08:36,450
所以这样的一个优化有什么好处呢？

120
00:08:36,455 --> 00:08:41,520
旧版本里面，它是一对一的关系，所以我会如果一个TiDB Server

121
00:08:41,580 --> 00:08:46,890
写入量比较大的话，会出现负载不均衡，容易出现性能的瓶颈

122
00:08:46,920 --> 00:08:48,330
在新版本里面

123
00:08:48,420 --> 00:08:51,720
我Pump是一个集群，TiDB Server只管去往里写

124
00:08:51,930 --> 00:08:55,380
Pump Cluster会做一个自动的一个负载均衡，所以

125
00:08:55,560 --> 00:08:59,880
相对第一种架构的话，

126
00:08:59,910 --> 00:09:01,890
负载是均衡的

127
00:09:06,600 --> 00:09:07,200


128
00:09:08,310 --> 00:09:13,620
在我commit时，在我写binlog的时候commit阶段，我会去

129
00:09:13,625 --> 00:09:14,820
发送

130
00:09:14,910 --> 00:09:18,420
这个commit怎么去写到哪个Pump里面了

131
00:09:18,630 --> 00:09:23,940
在Pump Cluster里面，它是有有要求的，就是说我prepare write选择哪一个

132
00:09:23,945 --> 00:09:24,690
Pump

133
00:09:24,720 --> 00:09:26,670
然后我commit这个

134
00:09:26,760 --> 00:09:29,340
命令写的时候也需要去写到对应的

135
00:09:29,550 --> 00:09:34,860
Pump里面去，这样的话，我们这个Pump，它会去做一个binlog的拼接，我会把

136
00:09:34,865 --> 00:09:40,170
我这条事务的prepare write和我commit的组装起来作为一个完整的

137
00:09:40,920 --> 00:09:41,790
binlog

138
00:09:43,590 --> 00:09:48,900
如果我在commit的阶段出现异常的话，Pump不是立刻就

139
00:09:48,905 --> 00:09:52,290
就会终止，它会去做一个尝试，尝试的

140
00:09:52,530 --> 00:09:54,990
过程最多是十分钟

141
00:09:55,320 --> 00:10:00,630
如果在最终阶段commit提交失败的话，Pump会去查TiKV里面的这个事务的

142
00:10:00,780 --> 00:10:02,010
commit_ts

143
00:10:02,100 --> 00:10:03,510
如果查到的话

144
00:10:03,540 --> 00:10:08,850
Pump就会去更新自己里面的commit_ts，如果没有查到的话

145
00:10:08,855 --> 00:10:10,620
这个事务就会被丢掉

146
00:10:15,330 --> 00:10:15,930


147
00:10:16,410 --> 00:10:19,050
前面讲了，我们讲了

148
00:10:19,080 --> 00:10:21,450
怎么去写？以及往哪里写？

149
00:10:21,720 --> 00:10:24,600
下面我们讲一下怎么去存binlog

150
00:10:24,780 --> 00:10:28,200
我的Pump已经写入了binlog，但是binlog怎么存呢？

151
00:10:28,500 --> 00:10:32,730
在Pump里面，binlog主要是它是以一个append

152
00:10:33,630 --> 00:10:38,940
的方式去写，它是类似一种写字的方式，我不断地去往后面去追加

153
00:10:39,150 --> 00:10:42,990
然后我写入成功之后返回一个success message

154
00:10:43,020 --> 00:10:44,460
给TiDB Server

155
00:10:44,760 --> 00:10:48,150
来说明我这个binlog是写入成功的

156
00:10:49,050 --> 00:10:54,360
其次在Pump里面，它会保存binlog的一些原信息，包括start-ts、commit-ts

157
00:10:54,365 --> 00:10:57,690
以及binlog的类型

158
00:10:58,920 --> 00:11:03,060
在TiDB里面，我们的binlog类型是一个类似MySQL的row模式

159
00:11:06,960 --> 00:11:11,430
Pump还有一个作用就是我刚刚介绍过的，它会去一个组装binlog

160
00:11:15,750 --> 00:11:21,060
我们现在数据写到了Pump里面，那我们怎么去把binlog同步到下游呢？

161
00:11:21,750 --> 00:11:24,270
同步到下游的工作是由drainer完成的

162
00:11:24,275 --> 00:11:28,230
其中，drainer的下游，我可以去接MySQL和TiDB

163
00:11:28,235 --> 00:11:29,520
或者去接kafka

164
00:11:29,670 --> 00:11:34,980
或者是输出到一个静态的文件里面，就是我

165
00:11:34,985 --> 00:11:37,200
们TiDB里面的pb文件

166
00:11:39,840 --> 00:11:45,150
因为TiDB是分布式去实现的，所以TiDB每个节点都会产生binlog

167
00:11:45,720 --> 00:11:48,780
每个Pump拉到了binlog之后它

168
00:11:48,990 --> 00:11:49,920
如果

169
00:11:50,550 --> 00:11:53,010
要实现

170
00:11:53,015 --> 00:11:57,750
集群整个完整的一个数据的话，它是需要把所有的binlog合在一起

171
00:11:58,320 --> 00:11:59,220
所以

172
00:11:59,850 --> 00:12:05,160
在drainer里面就有一个作用，一个merge，它需要把所有的Pump

173
00:12:05,190 --> 00:12:07,830
所以Pump节点拉到的数据

174
00:12:08,010 --> 00:12:13,320
去做一个合并，在drainer里面它是会去Pump拉第一条数据

175
00:12:13,325 --> 00:12:16,950
去进行commit-ts做一个比较

176
00:12:16,955 --> 00:12:18,810
按照大小去做一个

177
00:12:18,960 --> 00:12:19,830
合并

178
00:12:21,420 --> 00:12:23,760
还有一个就是我们刚才介绍的一个
按照大小去做一个合并，还有一个就是刚才提到的
179
00:12:24,090 --> 00:12:26,280
fake binlog机制

180
00:12:26,640 --> 00:12:31,950
这个主要的作用是某一个Pump如果在某段时间内一直没有写入的话

181
00:12:32,430 --> 00:12:33,570
如果我

182
00:12:33,750 --> 00:12:39,060
的 drainer 要去合并，每个Pump节点所有的binlog的话，因为

183
00:12:39,065 --> 00:12:44,370
一直等不到某一个节点的Pump的binlog，所以会去等待，为了防止

184
00:12:44,375 --> 00:12:49,680
等待时间过久的话，我们每每隔一段时间会去发送一个假的

185
00:12:49,685 --> 00:12:53,730
binlog，就说只带一个ts的binlog，这样的话我每个

186
00:12:54,150 --> 00:12:58,290
Pump节点都会去拿到一个commit-ts

187
00:12:58,560 --> 00:13:01,800
这样的话就可以方便去做一个是一个

188
00:13:02,070 --> 00:13:03,930
大小的一个排序和合并

189
00:13:05,760 --> 00:13:08,190
第三个是一个新增一个Pump节点

190
00:13:08,220 --> 00:13:09,930
Pump节点是我的一个处理

191
00:13:10,200 --> 00:13:12,870
如果我突然间新增一个Pump节点之后

192
00:13:12,990 --> 00:13:18,300
我也会去上游拉TiDB Server的一个binlog数据，但是因为我drainer

193
00:13:18,305 --> 00:13:21,030
不知道已经我已经新增了一个Pump节点

194
00:13:21,035 --> 00:13:24,930
所以这时候去做数据binlog的合并的时候

195
00:13:25,740 --> 00:13:28,470
我会忽略掉这个新增的Pump节点

196
00:13:28,560 --> 00:13:30,330
所以我这个时候合并的

197
00:13:30,450 --> 00:13:33,900
Pump的binlog信息其实是不完整的

198
00:13:34,110 --> 00:13:39,420
所以现在我们新增了一个Pump List，这样的话就是每一个Pump新上线的时候

199
00:13:39,930 --> 00:13:45,030
会是需要通知drainer我已经上线了，这样的时候

200
00:13:46,320 --> 00:13:49,290
每个Pump拉的信息才会真正的去

201
00:13:49,380 --> 00:13:52,440
给drainer识别，然后做一个数据的

202
00:13:52,620 --> 00:13:54,600
大小的一个排序和合并

203
00:13:57,630 --> 00:14:02,940
上面我们讲到了我drainer已经拉到了数据，拉到了数据我如果已经发送到下游的MySQL

204
00:14:02,945 --> 00:14:03,570


205
00:14:03,690 --> 00:14:09,000
TiDB或者kafka，或者我输出到一个pb文件里面去了，那我们对

206
00:14:09,005 --> 00:14:09,810
已经

207
00:14:11,040 --> 00:14:14,730
应用过的binlog我不可能一直存在我的本地

208
00:14:14,735 --> 00:14:16,680
所以就需要一个清理机制

209
00:14:17,670 --> 00:14:19,350
在TiDB binlog里面

210
00:14:19,680 --> 00:14:22,350
清理机制主要是由Pump去完成的

211
00:14:22,440 --> 00:14:26,070
因为我Pump拉下来数据之后，我会存一份的数据

212
00:14:26,100 --> 00:14:28,050
存一份的同时会

213
00:14:28,200 --> 00:14:33,030
去往下游drainer去发，所以数据的binlog主要是存在Pump本地

214
00:14:33,990 --> 00:14:35,850
在清理机制方面

215
00:14:35,970 --> 00:14:37,980
有两种方式，一种是

216
00:14:38,310 --> 00:14:40,590
gc默认的5天或者7天

217
00:14:42,240 --> 00:14:45,330
然后每一小时去检测一次，如果对已经

218
00:14:45,510 --> 00:14:47,850
满足条件的话，我会去做一个GC

219
00:14:48,510 --> 00:14:53,820
但是这种机制有个限制就是说我只有被drainer消费之后，我才可以被GC

220
00:14:53,850 --> 00:14:55,830
所以，如果drainer出现异常

221
00:14:55,920 --> 00:14:57,060
服务异常

222
00:14:57,120 --> 00:15:01,170
暂停或者下线的时候，我会造成binlog的堆积

223
00:15:01,770 --> 00:15:05,280
第二种方式去手工删除已经消费了的binlog

224
00:15:05,340 --> 00:15:10,500
因为这种方式是有风险的，手工删除的话，可能对一些

225
00:15:10,590 --> 00:15:15,270
checkpoint把握不是很准，所以会经常会出现一些

226
00:15:15,420 --> 00:15:19,140
风险问题，所以这种方式我们是不推荐的

227
00:15:23,760 --> 00:15:26,790
刚才我们讲到了binlog的产生以及

228
00:15:26,880 --> 00:15:28,410
它的一些

229
00:15:28,440 --> 00:15:30,060
怎么去同步的

230
00:15:30,270 --> 00:15:34,200
过程，那对于我整个集群要做数据的备份

231
00:15:34,290 --> 00:15:35,640
我去怎么做呢？

232
00:15:36,630 --> 00:15:41,460
在总TiDB的备份的时候，我们首先是需要一个全量的备份

233
00:15:41,520 --> 00:15:46,830
全量备份，怎么做呢？我们可以用TiDB提供的Mydumper去做一个数据的全量

234
00:15:46,835 --> 00:15:47,730
的导出

235
00:15:48,090 --> 00:15:49,620
这样的话，我们可以

236
00:15:49,950 --> 00:15:55,260
自己写一个自动化的脚本，每周或者每一个月进行一次数据的全量备份

237
00:15:55,950 --> 00:15:58,710
备份之后，我们去做一个

238
00:15:59,040 --> 00:16:02,010
一个打包压缩，然后做一个归档

239
00:16:02,400 --> 00:16:07,710
其次，对增量的数据呢，我们因为我之前已经讲过一种drainer可以输出到

240
00:16:07,715 --> 00:16:09,450
一个pb文件里面

241
00:16:09,570 --> 00:16:14,880
去把binlog静态的保存下来，所以呢，我们在做备份的时候可以用mydumper

242
00:16:15,390 --> 00:16:20,700
全量的数据加上一个drainer输出的pb文件的一个增量的数据去做一个

243
00:16:20,970 --> 00:16:22,860
整个集群的完整的一个备份

244
00:16:25,410 --> 00:16:30,720
我们刚才介绍的数据的一个增量备份的时候会把binlog输出到一个

245
00:16:31,020 --> 00:16:36,270
pb文件里面,这个文件是是一个二进制格式的binlog文件

246
00:16:36,450 --> 00:16:41,760
是我们没办法去直接去解析的，所以就需要一种

247
00:16:41,765 --> 00:16:42,960
工具去解析

248
00:16:44,130 --> 00:16:49,440
然后我们官方提供的reparo这个工具可以去读取pb file，然后去把它

249
00:16:49,445 --> 00:16:53,430
binlog解析出来恢复到TiDB或者MySQL数据库

250
00:16:54,540 --> 00:16:59,850
这个工具有两个特点，大家可以多关注一下，一个是支持指定的schema或者

251
00:16:59,855 --> 00:17:05,160
table的一个解析就是我这个binlog里面数据，我解析出来之后

252
00:17:05,165 --> 00:17:08,790
可以指定我只要某一个schema或者table的数据

253
00:17:09,000 --> 00:17:14,310
另外一个方面就是制定指定时间点的恢复，我可以指定到我只

254
00:17:14,315 --> 00:17:16,650
需要某一个时间点的数据

255
00:17:20,250 --> 00:17:25,560
刚才我们讲了大概的数据的一个恢复就是全量的Mydumper加drainer

256
00:17:26,160 --> 00:17:29,430
输出到pb文件的binlog去做一个增量的恢复

257
00:17:29,670 --> 00:17:31,650
然后这里面我们讲一下

258
00:17:32,010 --> 00:17:37,320
恢复的一些特点，比如我们支持全量的逻辑备份，多种数据库格式的增量备份

259
00:17:38,040 --> 00:17:43,350
支持按时间点，就是我们通过reparo工具解析出binlog去做一个支持

260
00:17:43,355 --> 00:17:45,000
时间点的一个恢复

261
00:17:45,180 --> 00:17:48,150
以及按库或表进行数据的恢复

262
00:17:49,260 --> 00:17:54,570
另外一个特点就是我们需要注意一下TiDB的备份数据，我们可以恢复到TiDB集群

263
00:17:54,750 --> 00:17:59,280
或者是MySQL 5.7.x小版本的数据库里面

264
00:18:02,190 --> 00:18:02,790


265
00:18:04,320 --> 00:18:09,630
以上就是我们今天要讲的TiDB备份恢复的所有内容，如果大家还有疑问的话

266
00:18:09,635 --> 00:18:12,420
可以去，登陆我们的平台和官方网站

267
00:18:12,425 --> 00:18:13,140
去

268
00:18:13,290 --> 00:18:14,490
学习

269
00:18:19,200 --> 00:18:19,800
